{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "\n",
    "def read_csv(name):\n",
    "    \"\"\"\"read a file and convert it to a pd dataframe\"\"\"\n",
    "    files = os.listdir(\"./\")\n",
    "    for file in files:\n",
    "        # file_name = file.rsplit('.',1)[0]\n",
    "        if file.endswith(\".csv\") and file == name:\n",
    "            raw_df = pd.read_csv(os.path.join(\"./\",file), header= None)\n",
    "            header = [\"SegmentNr\", \"Position\", \"A\", \"C\", \"G\", \"T\"]\n",
    "            raw_df.columns = header\n",
    "            raw_df = raw_df.reset_index(drop = True)\n",
    "            k = file.rsplit('.',1)[0][-1]\n",
    "            k = int(k)\n",
    "            return raw_df, k, name\n",
    "    else:\n",
    "        raise LookupError(\"This file is not existing\")\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data_frame):\n",
    "        # cleaning step1: ( number of positions are less than max of the position)\n",
    "        # redundant data with similar data\n",
    "        data_frame = data_frame.drop_duplicates()\n",
    "        \n",
    "        cl1 = data_frame.groupby(\"SegmentNr\").agg({\"Position\" : [\"max\", \"nunique\"]})\n",
    "        cl1 = cl1.reset_index()\n",
    "        error_data = cl1[cl1[\"Position\"][\"max\"] > cl1[\"Position\"][\"nunique\"]][\"SegmentNr\"]\n",
    "        to_del = data_frame[data_frame[\"SegmentNr\"].isin(error_data)].index\n",
    "        data_frame = data_frame.drop(to_del)\n",
    "\n",
    "        # cleaning step2:\n",
    "\n",
    "        cl2 = data_frame.groupby([\"SegmentNr\", \"Position\"]).agg({\"A\": [\"count\",\"max\", \"min\"],\\\n",
    "                \"C\": [\"max\", \"min\"], \"G\": [\"max\", \"min\"],\\\n",
    "                        \"T\": [\"max\", \"min\"]}).reset_index()\n",
    "        cl2.columns = [''.join(col).strip() for col in cl2.columns.values]\n",
    "\n",
    "        # redundant data\n",
    "        cl2 = cl2[cl2[\"Acount\"] > 1]\n",
    "\n",
    "        # redundant data with different values\n",
    "        cl_diff = cl2[((cl2[\"Amax\"] != cl2[\"Amin\"]) | \n",
    "                (cl2[\"Cmax\"] != cl2[\"Cmin\"]) | \n",
    "                (cl2[\"Gmax\"] != cl2[\"Gmin\"]) | \n",
    "                (cl2[\"Tmax\"] != cl2[\"Tmin\"]))]\n",
    "        to_del = data_frame[data_frame[\"SegmentNr\"].isin(cl_diff[\"SegmentNr\"])].index\n",
    "        data_frame = data_frame.drop(to_del)\n",
    "\n",
    "        # cleaning step 3:\n",
    "        cl3 = data_frame.query(\"(A + C + G + T) != 1\")\n",
    "        to_del = data_frame[data_frame[\"SegmentNr\"].isin(cl3[\"SegmentNr\"])].index\n",
    "        data_frame = data_frame.drop(to_del)\n",
    "    \n",
    "        # cleaning step 4:\n",
    "        cl4 = data_frame.copy()\n",
    "        cl4[\"Id\"] = cl4[[\"Position\", \"A\", \"C\", \"G\", \"T\"]].astype(str).agg(''.join, axis = 1)\n",
    "        cl4 = cl4.groupby(\"SegmentNr\").agg({\"Id\": lambda x: '\\\\'.join(x)}).reset_index()\n",
    "        dups = cl4[cl4.duplicated(\"Id\")]\n",
    "        to_del =data_frame[data_frame[\"SegmentNr\"].isin(dups[\"SegmentNr\"])].index\n",
    "        data_frame = data_frame.drop(to_del)\n",
    "        data_frame = data_frame.reset_index(drop=True)\n",
    "        return data_frame\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df, k, filename \u001b[38;5;241m=\u001b[39m read_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDNA_3_3.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m clean_data(df)\n",
      "Cell \u001b[0;32mIn[169], line 11\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# file_name = file.rsplit('.',1)[0]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file \u001b[38;5;241m==\u001b[39m name:\n\u001b[0;32m---> 11\u001b[0m         raw_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m,file), header\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m         header \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentNr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m         raw_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m header\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:550\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:758\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2021\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "df, k, filename = read_csv(\"DNA_3_3.csv\")\n",
    "df = clean_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(df):\n",
    "    df[\"segment\"] = df[['A', 'C', 'G', 'T']].idxmax(axis=1)\n",
    "    df = df.sort_values(['SegmentNr', 'Position'])\n",
    "    segment = df.groupby(\"SegmentNr\").agg({\"segment\": lambda x: ''.join(x)}).reset_index()\n",
    "    seg_json = segment.to_json(orient='records')\n",
    "    seg_json = json.loads(seg_json)\n",
    "    return seg_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "\n",
    "df = generate_sequences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGGT\n",
      "TTGG\n",
      "GTTT\n"
     ]
    }
   ],
   "source": [
    "# def construct_graph(json_data, k):\n",
    "for i in df:\n",
    "    print(i['segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(df, k):\n",
    "    G = nx.MultiDiGraph()\n",
    "    for i in df:\n",
    "        seg = i['segment']\n",
    "        steps = len(seg) - k\n",
    "        k_mers = []\n",
    "        for i in range(steps + 1):\n",
    "            k_mers.append(seg[i:i+k])\n",
    "\n",
    "        for i in k_mers:\n",
    "            l_seg = i[:k-1]\n",
    "            r_seg = i[1:k+1]\n",
    "            G.add_edge(l_seg, r_seg)\n",
    "    return G   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = construct_graph(df,k)\n",
    "# list(df.edges(keys=False))\n",
    "\n",
    "edges = [('ATTA', 'TTAC'), ('TTAC', 'TACT'), ('TACT', 'ACTC'),\n",
    "             ('ACTC', 'CTCG'), ('CTCG', 'TCGC'), ('TCGC', 'CGCT'), \n",
    "             ('CGCT', 'GCTA'), ('TCGC', 'GCTA')]\n",
    "debruijn_graph = nx.MultiDiGraph()\n",
    "for edge in edges:\n",
    "    debruijn_graph.add_edge(edge[0], edge[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'SegmentNr': 1, 'segment': 'GGGT'},\n",
       " {'SegmentNr': 2, 'segment': 'TTGG'},\n",
       " {'SegmentNr': 3, 'segment': 'GTTT'}]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "grrr = construct_graph(df, k)        \n",
    "\n",
    "def orthogonal_layout(G):\n",
    "    pos = {}\n",
    "    nodes = list(G.nodes)\n",
    "    graph_size = int(len(nodes)**0.5) + (len(nodes) % int(len(nodes)**0.5) != 0)\n",
    "\n",
    "    for i, node in enumerate(nodes):\n",
    "        row = i // graph_size\n",
    "        col = i % graph_size\n",
    "        pos[node] = (col, row)\n",
    "    return pos\n",
    "\n",
    "def plot_graph(G, filename):\n",
    "    fig = plt.figure()\n",
    "    pos = orthogonal_layout(G)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightgreen')\n",
    "    nx.draw_networkx_labels(G, pos, font_size=6, font_color='black', font_weight='bold')\n",
    "    \n",
    "    # for separating edges from each other\n",
    "    for (u, v, key) in G.edges(keys=True):\n",
    "        num_edges = G.number_of_edges(u, v)\n",
    "        index = list(G[u][v]).index(key)\n",
    "        placeholder = (index - num_edges / 2) * 0.2\n",
    "        rad = 0.2 + placeholder\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], connectionstyle=f'arc3,rad={rad}')\n",
    "    plt.savefig(filename, format='png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "plot_graph(grrr, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_connect(graph):\n",
    "    visited = set()\n",
    "    stack = []\n",
    "    v = list(graph.nodes)[0]\n",
    "    stack.append(v)\n",
    "    while stack:\n",
    "        v = stack.pop()\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            neighbours = list(graph[v])\n",
    "            neighbours.reverse()\n",
    "            for w in neighbours:\n",
    "                stack.append(w)\n",
    "    if len(visited) < len(graph):\n",
    "        return False            \n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def is_valid_graph(graph):    \n",
    "    # check if graph is connected\n",
    "    if not is_connect(graph):\n",
    "        return False\n",
    "    else:\n",
    "        # check if number of nodes with diffent number of in and out degrees are 0 or 2\n",
    "        # also check if there are 2 nodes in above list they sould be start and end points\n",
    "        different_in_out = []\n",
    "        for node in graph.nodes():\n",
    "            if graph.out_degree(node) != graph.in_degree(node):\n",
    "                different_in_out.append(graph.out_degree(node) - graph.in_degree(node))\n",
    "        if len(different_in_out) not in [0,2]:\n",
    "            return False\n",
    "        elif len(different_in_out) == 2 and (different_in_out[0]\\\n",
    "            + different_in_out[1] != 0 or different_in_out[0]*different_in_out[1] != -1):\n",
    "            return False                \n",
    "        return True\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_graph(grrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eulerian_path_gen(graph):\n",
    "    start_node =''\n",
    "    for node in graph.nodes():\n",
    "        if graph.out_degree(node) - graph.in_degree(node) == 1:\n",
    "            start_node = node\n",
    "            break\n",
    "    if not start_node:\n",
    "        for node in graph.nodes():\n",
    "            if graph.out_degree(node) > 0:\n",
    "                start_node = node\n",
    "                break\n",
    "    print(f\"start from node{start_node}\")\n",
    "    G = graph.copy()\n",
    "    stack = []\n",
    "    path = []\n",
    "    v = start_node\n",
    "    stack.append(v)\n",
    "    out_edges = dict(G.out_degree())\n",
    "    while stack:\n",
    "        v = stack[-1]\n",
    "        if out_edges[v] == 0:\n",
    "            if len(stack) > 1:\n",
    "                u = stack[-2]\n",
    "                path.insert(0,(u,v))\n",
    "            stack.pop()\n",
    "        else:\n",
    "            # sort the edges based on name of the destination node\n",
    "            edges = sorted(G.edges(v, keys=True), key=lambda x: x[1])\n",
    "            for _, w, key in edges:\n",
    "                stack.append(w)\n",
    "                out_edges[v] -=1\n",
    "                G.remove_edge(v, w, key=key)\n",
    "                break\n",
    "    return list(path)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dna_sequence(graph):\n",
    "    path = Eulerian_path_gen(graph)\n",
    "    # path = list(nx.eulerian_path(graph))\n",
    "    DNA = path[0][0]\n",
    "    for i in path:\n",
    "        DNA += i[1][-1]\n",
    "    print(path)\n",
    "    return DNA\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start from nodeGG\n",
      "[('GG', 'GG'), ('GG', 'GT'), ('GT', 'TT'), ('TT', 'TT'), ('TT', 'TG'), ('TG', 'GG')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GGGTTTGG'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNA_edge_list = [('GG', 'GG'), ('GG', 'GT'), ('GT', 'TT'),('TT', 'TT'),\n",
    "                 ('TT', 'TG'), ('TG', 'GG')]\n",
    "debruijn_graph = nx.MultiDiGraph()\n",
    "for edge in DNA_edge_list:\n",
    "    debruijn_graph.add_edge(edge[0], edge[1])\n",
    "\n",
    "construct_dna_sequence(debruijn_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TTAA', 'TAAT'), ('TAAT', 'AATT'), ('AATT', 'ATTA'), ('ATTA', 'TTAC'), ('TTAC', 'TACT'), ('TACT', 'ACTC'), ('ACTC', 'CTCA'), ('CTCA', 'TCAC'), ('TCAC', 'CACT'), ('CACT', 'ACTA'), ('ACTA', 'CTAC'), ('CTAC', 'TACG'), ('TACG', 'ACGC'), ('ACGC', 'CGCA'), ('CGCA', 'GCAC'), ('GCAC', 'CACT'), ('CACT', 'ACTG'), ('ACTG', 'CTGG'), ('CTGG', 'TGGC'), ('TGGC', 'GGCT'), ('GGCT', 'GCTA'), ('GCTA', 'CTAA'), ('CTAA', 'TAAT'), ('TAAT', 'AATT'), ('AATT', 'ATTA'), ('ATTA', 'TTAC'), ('TTAC', 'TACT'), ('TACT', 'ACTC'), ('ACTC', 'CTCA'), ('CTCA', 'TCAC'), ('TCAC', 'CACT'), ('CACT', 'ACTG'), ('ACTG', 'CTGG'), ('CTGG', 'TGGG'), ('TGGG', 'GGGT'), ('GGGT', 'GGTC'), ('GGTC', 'GTCA'), ('GTCA', 'TCAC'), ('TCAC', 'CACT'), ('CACT', 'ACTG')]\n"
     ]
    }
   ],
   "source": [
    "dna  = construct_dna_sequence(grrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(s, filename):\n",
    "    file = filename + \".txt\"\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output(dna, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TTAA', 'TAAT')\n",
      "('TAAT', 'AATT')\n",
      "('AATT', 'ATTA')\n",
      "('ATTA', 'TTAC')\n",
      "('TTAC', 'TACT')\n",
      "('TACT', 'ACTC')\n",
      "('ACTC', 'CTCA')\n",
      "('CTCA', 'TCAC')\n",
      "('TCAC', 'CACT')\n",
      "('CACT', 'ACTA')\n",
      "('ACTA', 'CTAC')\n",
      "('CTAC', 'TACG')\n",
      "('TACG', 'ACGC')\n",
      "('ACGC', 'CGCA')\n",
      "('CGCA', 'GCAC')\n",
      "('GCAC', 'CACT')\n",
      "('CACT', 'ACTG')\n",
      "('ACTG', 'CTGG')\n",
      "('CTGG', 'TGGG')\n",
      "('TGGG', 'GGGT')\n",
      "('GGGT', 'GGTC')\n",
      "('GGTC', 'GTCA')\n",
      "('GTCA', 'TCAC')\n",
      "('TCAC', 'CACT')\n",
      "('CACT', 'ACTG')\n",
      "('ACTG', 'CTGG')\n",
      "('CTGG', 'TGGC')\n",
      "('TGGC', 'GGCT')\n",
      "('GGCT', 'GCTA')\n",
      "('GCTA', 'CTAA')\n",
      "('CTAA', 'TAAT')\n",
      "('TAAT', 'AATT')\n",
      "('AATT', 'ATTA')\n",
      "('ATTA', 'TTAC')\n",
      "('TTAC', 'TACT')\n",
      "('TACT', 'ACTC')\n",
      "('ACTC', 'CTCA')\n",
      "('CTCA', 'TCAC')\n",
      "('TCAC', 'CACT')\n",
      "('CACT', 'ACTG')\n"
     ]
    }
   ],
   "source": [
    "euler_path_nx = list(nx.eulerian_path(grrr))\n",
    "for i in euler_path_nx:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutMultiEdgeDataView([('TAAT', 'AATT', 0), ('TAAT', 'AATT', 1)])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grrr.edges('TAAT',keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
